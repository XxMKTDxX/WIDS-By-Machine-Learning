{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cat *.csv > data.csv"
      ],
      "metadata": {
        "id": "sNPt187eKh5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hZ6e3RS7cp5",
        "outputId": "f3c84581-adaf-41ec-d1cc-a7c176d19bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 0: `cat /content/CSV/3.(Re)Assoc/*.csv > 3.csv'\n"
          ]
        }
      ],
      "source": [
        "!cat /content/CSV/1.Deauth/*.csv > 1.csv\n",
        "!cat /content/CSV/2.Disas/*.csv > 2.csv\n",
        "!cat /content/CSV/3.(Re)Assoc/*.csv > 3.csv\n",
        "!cat /content/CSV/4.Rogue_AP/*.csv > 4.csv\n",
        "!cat /content/CSV/5.Krack/*.csv > 5.csv\n",
        "!cat /content/CSV/6.Kr00k/*.csv > 6.csv\n",
        "!cat /content/CSV/7.SSH/*.csv > 7.csv\n",
        "!cat /content/CSV/8.Botnet/*.csv > 8.csv\n",
        "!cat /content/CSV/9.Malware/*.csv > 9.csv\n",
        "!cat /content/CSV/10.SQL_Injection/*.csv > 10.csv\n",
        "!cat /content/CSV/11.SSDP/*.csv > 11.csv\n",
        "!cat /content/CSV/12.Evil_Twin/*.csv > 12.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp55fnZm7sF9",
        "outputId": "b9dae4d8-31ee-4a8c-f613-a51fb2a8eaea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://icsdweb.aegean.gr/awid/download.php?token=195ca45fd9c1f206318db7d755dd74f64a0997e6554bc220bbefae730b2f7cd5 -O DATASET.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzP2jpeD7vNk",
        "outputId": "49b1343e-8e58-4534-8695-fed6c2ba2358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 20:17:51--  https://icsdweb.aegean.gr/awid/download.php?token=195ca45fd9c1f206318db7d755dd74f64a0997e6554bc220bbefae730b2f7cd5\n",
            "Resolving icsdweb.aegean.gr (icsdweb.aegean.gr)... 195.251.134.102\n",
            "Connecting to icsdweb.aegean.gr (icsdweb.aegean.gr)|195.251.134.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14471343012 (13G) [application/octet-stream]\n",
            "Saving to: ‘DATASET.tar.gz’\n",
            "\n",
            "DATASET.tar.gz      100%[===================>]  13.48G  9.55MB/s    in 25m 8s  \n",
            "\n",
            "2023-04-27 20:43:00 (9.15 MB/s) - ‘DATASET.tar.gz’ saved [14471343012/14471343012]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/DATASET.zip"
      ],
      "metadata": {
        "id": "m4R6K0MX7ya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(file_path):\n",
        "    '''Fuction to load csv data file as dataframe from given file path'''\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "df = load_data('/content/data.csv')\n",
        "print('Shape of data: ', df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lnftzI9VAWCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "z8jQh4ILAWAN",
        "outputId": "de489c36-1966-4581-fcb5-2e0e58b343ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea8415b8a3ee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def class_distrib(df):\n",
        "    '''This function displays class distribution as a bar plot for the given dataframe'''\n",
        "    unique, counts = np.unique(df['class'], return_counts=True)\n",
        "    distrib = dict(zip(unique, counts))\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x=list(distrib.keys()), y=list(distrib.values()))\n",
        "    plt.title(\"Class Distribution\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Count\")\n",
        "\n",
        "    plt.show()\n",
        "    print(distrib)\n",
        "    \n",
        "class_distrib(df)"
      ],
      "metadata": {
        "id": "ka0Av20BAV9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def subset_sample(df, class_name, n):\n",
        "    '''This function extracts n range subset from the given dataframe for the given input class'''\n",
        "    df_maj = df[df['class'] == class_name]\n",
        "    df = df[df['class'] != class_name]\n",
        "\n",
        "    df_maj = df_maj[100 : n]\n",
        "\n",
        "    df = pd.concat([df_maj, df], axis=0)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = subset_sample(df, 'spoof', 200000)\n",
        "class_distrib(df)"
      ],
      "metadata": {
        "id": "Rhp88XV1AV7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rows which still have some NaN value \n",
        "df_nan_rows = df[df.isnull().any(axis=1)]\n",
        "print('All class rows which still have some NaN value: ', len(df_nan_rows))\n",
        "\n",
        "df_nan_rows_normal = df[df.isnull().any(axis=1)][df['class'] == 'normal']\n",
        "print('Normal class rows which still have some NaN value: ', len(df_nan_rows_normal))"
      ],
      "metadata": {
        "id": "l2ULkfVDAV41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shallow_clean(df):\n",
        "    '''Cleans the the given dataframe at a shallow level'''\n",
        "    # Remove columns with >= 85% NaNs\n",
        "    thres_limit = len(df) * 0.25\n",
        "    df = df.dropna(thresh=int(thres_limit), axis=1)\n",
        "\n",
        "    # Remove columns with constant value\n",
        "    df = df.drop(columns=df.columns[df.nunique()==1])\n",
        "\n",
        "    # Remove rows with >= 50% NaNs\n",
        "    thres_limit = df.shape[1] * 0.50\n",
        "    df = df.dropna(thresh=int(thres_limit), axis=0)\n",
        "    \n",
        "    return df\n",
        "\n",
        "cols_before = set(df.columns)\n",
        "df = shallow_clean(df)\n",
        "cols_after = set(df.columns)\n",
        "print('Columns removed:', cols_before - cols_after)"
      ],
      "metadata": {
        "id": "Yw5USVjhAV2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().any()"
      ],
      "metadata": {
        "id": "6k2R3k8bAVzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "FelpfkBPAVqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_distrib(df_nan_rows)\n"
      ],
      "metadata": {
        "id": "RLc2cKT1Atxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill the NULL values\n",
        "num_nan_cols = ['radiotap.mactime', 'radiotap.datarate', 'wlan.frag', 'wlan.seq']\n",
        "for c in num_nan_cols:\n",
        "    df[c].fillna(value=df[c].mean(), inplace=True)\n",
        "    \n",
        "cat_nan_cols = ['wlan.ra', 'wlan.da', 'wlan.ta', 'wlan.bssid', 'wlan.sa']\n",
        "for c in cat_nan_cols:\n",
        "    df[c].fillna(value='null', inplace=True)\n",
        "    \n",
        "df.columns[df.isna().any()].tolist()"
      ],
      "metadata": {
        "id": "4Sr1F-uaAtu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_drop = ['wlan.da', 'wlan.ra', 'wlan.ta', 'wlan.sa', 'wlan.bssid', 'radiotap.dbm_antsignal']\n",
        "df = df.drop(cols_drop, axis=1)"
      ],
      "metadata": {
        "id": "uJewBEPrAtsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save processed data to file\n",
        "df.to_csv('data/train_processed_1.csv', index=False)"
      ],
      "metadata": {
        "id": "vii4qydhAtpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load processed data to df\n",
        "df = pd.read_csv('data/train_processed_1.csv')"
      ],
      "metadata": {
        "id": "odnaeCeaAtm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df.drop(['class'], axis=1)\n",
        "y = df['class']"
      ],
      "metadata": {
        "id": "QjyUDSRsFepL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "std_cols = ['frame.time_epoch', 'frame.time_delta', 'frame.time_delta_displayed', 'frame.time_relative', 'frame.len',\n",
        "           'frame.cap_len', 'radiotap.length', 'radiotap.mactime', 'radiotap.datarate', 'radiotap.channel.freq',\n",
        "           'wlan.duration', 'wlan.frag', 'wlan.seq']\n",
        "with open('scaler_save/columns.txt', 'w') as f:\n",
        "    [f.write(c+'\\n') for c in std_cols]\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X[std_cols] = standard_scaler.fit_transform(X[std_cols])\n",
        "\n",
        "# Save this scalar to file\n",
        "joblib.dump(standard_scaler, 'scaler_save/scaler.gz')"
      ],
      "metadata": {
        "id": "_XLWALXtFemw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc_cols = ['wlan.fc.type_subtype', 'wlan.fc.type', 'wlan.fc.subtype', 'wlan.fc.ds', 'radiotap.present.tsft']\n",
        "\n",
        "with open('encoder_save/columns.txt', 'w') as f:\n",
        "    [f.write(c+'\\n') for c in enc_cols]\n",
        "    \n",
        "oh_encoder_x = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "array_ohe = oh_encoder_x.fit_transform(X[enc_cols])\n",
        "df_ohe = pd.DataFrame(array_ohe, index=df.index)\n",
        "\n",
        "#Extract only the columns that didnt need to be encoded\n",
        "df_other = X.drop(columns=enc_cols)\n",
        "\n",
        "#Concatenate the two dataframes : \n",
        "X = pd.concat([df_ohe, df_other], axis=1)\n",
        "\n",
        "# Save this encoder to file\n",
        "joblib.dump(oh_encoder_x, 'encoder_save/encoder.gz')"
      ],
      "metadata": {
        "id": "Ghv1F6YGFekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Shape of y before encoding: ', y.shape)\n",
        "print('ReShape of y for encoding: ', y.values.reshape(-1, 1).shape)\n",
        "\n",
        "oh_encoder_y = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_categorical = oh_encoder_y.fit_transform(y.values.reshape(-1, 1))\n",
        "print('Shape of y after encoding: ', y_categorical.shape)"
      ],
      "metadata": {
        "id": "_fuEJuGlFehw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.shape, X.shape, y.shape, y_categorical.shape"
      ],
      "metadata": {
        "id": "54AlVescFefR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def time_series(X, chunk_size):\n",
        "    si = chunk_size\n",
        "    ei = len(X)\n",
        "    \n",
        "    data = []\n",
        "\n",
        "    for i in range(si, ei):\n",
        "        idx = range(i - chunk_size, i, 1)\n",
        "        data.append(X[idx])\n",
        "\n",
        "    data = np.array(data)\n",
        "    return data\n",
        "\n",
        "chunk_size = 10\n",
        "X_time = time_series(X.values, chunk_size)\n",
        "X_time.shape"
      ],
      "metadata": {
        "id": "mp8jmQSTFecc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_time, y_categorical[chunk_size:], test_size=0.33, stratify=y[chunk_size:])\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.33, stratify=y_val)"
      ],
      "metadata": {
        "id": "KQK4CDK1FeZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "relevant_columns = df.columns\n",
        "with open('data/relevant_columns.txt', 'w') as f:\n",
        "    [f.write(rc+'\\n') for rc in relevant_columns]"
      ],
      "metadata": {
        "id": "zCCv5kDcFeW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_labels = oh_encoder_y.categories_[0]\n",
        "with open('data/class_labels.txt', 'w') as f:\n",
        "    [f.write(cl+'\\n') for cl in class_labels]"
      ],
      "metadata": {
        "id": "3FOMcHLwFeS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train.shape[1:3]"
      ],
      "metadata": {
        "id": "KNRWH3r5FoDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_model(input_shape, n_class):\n",
        "    \"\"\"This function creates a simple DL model\"\"\"\n",
        "    input_layer = tf.keras.Input(shape=(input_shape))\n",
        "    \n",
        "    gru_layer = layers.GRU(16, return_sequences=False)(input_layer)\n",
        "    \n",
        "    dense_layer1 = layers.Dense(16, activation='relu')(gru_layer)\n",
        "    dense_layer2 = layers.Dense(32, activation='relu')(dense_layer1)\n",
        "    \n",
        "    output_layer = layers.Dense(n_class, activation='softmax')(dense_layer2)\n",
        "    return Model(input_layer, output_layer)\n",
        "    \n",
        "model = create_model(X_train.shape[1:3], len(oh_encoder_y.categories_[0]))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iyUYdfAEFoAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Ty19xonPFn9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "log_dir=\"tensorboard\\\\logs\"\n",
        "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath='model_save/best_model.h5', monitor='val_accuracy',\n",
        "                             verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=4, verbose=1)"
      ],
      "metadata": {
        "id": "05MzHF0QFn3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val),\n",
        "              batch_size=128, callbacks=[checkpoint, earlystop, tensorboard])"
      ],
      "metadata": {
        "id": "cngHlqoyFt_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXEU8bQdFt8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_preds_val = np.argmax(model.predict(X_val), axis=1)\n",
        "y_val = np.argmax(y_val, axis=1)\n",
        "y_preds_test = np.argmax(model.predict(X_test), axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "y_preds_train = np.argmax(model.predict(X_train), axis=1)\n",
        "y_train = np.argmax(y_train, axis=1)\n",
        "\n",
        "with open('data/class_labels.txt', 'r') as f:\n",
        "    class_labels = np.array([line.strip() for line in f])\n",
        "\n",
        "cm_val = confusion_matrix(class_labels[y_val], class_labels[y_preds_val])\n",
        "cm_test = confusion_matrix(class_labels[y_test], class_labels[y_preds_test])\n",
        "cm_train = confusion_matrix(class_labels[y_train], class_labels[y_preds_train])\n",
        "\n",
        "df_cm_val = pd.DataFrame(cm_val, columns=class_labels, index=class_labels)\n",
        "df_cm_train = pd.DataFrame(cm_train, columns=class_labels, index=class_labels)\n",
        "df_cm_test = pd.DataFrame(cm_test, columns=class_labels, index=class_labels)\n",
        "df_cm_val.index.name = df_cm_train.index.name = df_cm_test.index.name = 'Actual'\n",
        "df_cm_val.columns.name = df_cm_train.columns.name = df_cm_test.columns.name = 'Predicted'"
      ],
      "metadata": {
        "id": "KY_qNSF2Ft5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (14, 24))\n",
        "\n",
        "fig.add_subplot(3, 1, 1)\n",
        "sp2 = sns.heatmap(df_cm_train, cmap=\"Blues\", annot=True, fmt='g')\n",
        "sp2.set_title('Traning CM')\n",
        "\n",
        "fig.add_subplot(3, 1, 2)\n",
        "sp1 = sns.heatmap(df_cm_val, cmap=\"Blues\", annot=True, fmt='g')\n",
        "sp1.set_title('Validation CM')\n",
        "\n",
        "fig.add_subplot(3, 1, 3)\n",
        "sp2 = sns.heatmap(df_cm_test, cmap=\"Blues\", annot=True, fmt='g')\n",
        "sp2.set_title('Testing CM')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G4laRHKZFtyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(classification_report(class_labels[y_val], class_labels[y_preds_val], target_names=list(class_labels)))\n",
        "print(classification_report(class_labels[y_test], class_labels[y_preds_test], target_names=list(class_labels)))"
      ],
      "metadata": {
        "id": "RWCQmhSOGPk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}